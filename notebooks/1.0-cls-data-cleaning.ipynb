{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_values = pd.read_csv('../data/raw/training_values.csv')\n",
    "train_labels = pd.read_csv('../data/raw/training_labels.csv')\n",
    "train_df = train_values.merge(train_labels, on='id')\n",
    "\n",
    "test_values = pd.read_csv('../data/raw/test_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many features have illogical values (e.g. '0' as the name of the funder).  I am replacing these illogical values with nans, as the values were likely entered as a placeholder for 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train = train_df\n",
    "clean_test = test_values\n",
    "\n",
    "for df in [clean_train, clean_test]:\n",
    "    df['funder'].replace('0', np.nan, inplace=True)\n",
    "    df['installer'].replace('0', np.nan, inplace=True)\n",
    "    df['longitude'].replace(0, np.nan, inplace=True)\n",
    "    df['latitude'].replace(-0.00000002, np.nan, inplace=True)\n",
    "    df['population'].replace(0, np.nan, inplace=True)\n",
    "    df['construction_year'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the test features contain different conventions for capitalization (e.g. District Council vs. District council).  Thus, I am making all text values lowercase to ensure entries are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [clean_train, clean_test]:\n",
    "    for col in df:\n",
    "        if df[col].dtype == 'O' and col not in ['permit', 'public_meeting']:\n",
    "            df[col] = df[col].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the date recorded datetime, so that alogrithms can split on before/after a certain date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train['date_recorded'] = pd.to_datetime(clean_train['date_recorded'])\n",
    "clean_test['date_recorded'] = pd.to_datetime(clean_test['date_recorded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train['public_meeting'].fillna(value=True, inplace=True)\n",
    "clean_test['public_meeting'].fillna(value=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train['permit'].fillna(value=True, inplace=True)\n",
    "clean_test['permit'].fillna(value=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train['longitude'].fillna(value=clean_train['longitude'].mean(), inplace=True)\n",
    "clean_test['longitude'].fillna(value=clean_test['longitude'].mean(), inplace=True)\n",
    "clean_train['latitude'].fillna(value=clean_train['latitude'].mean(), inplace=True)\n",
    "clean_test['latitude'].fillna(value=clean_test['latitude'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummies_train = pd.get_dummies(clean_train[['basin', 'extraction_type', 'management']])\n",
    "clean_constr_train = pd.concat([clean_train, dummies_train], axis=1)\n",
    "clean_constr_train.drop(['funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region', 'lga', 'ward', 'scheme_management', 'scheme_name', 'extraction_type', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'source', 'source_type', 'waterpoint_type', 'recorded_by', 'extraction_type_group', 'extraction_type_class', 'quantity_group', 'source_class', 'waterpoint_type_group', 'region_code', 'district_code', 'status_group', 'id', 'population'], axis=1, inplace=True)\n",
    "dummies_test = pd.get_dummies(clean_test[['basin', 'extraction_type', 'management']])\n",
    "clean_constr_test = pd.concat([clean_test, dummies_test], axis=1)\n",
    "clean_constr_test.drop(['funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region', 'lga', 'ward', 'scheme_management', 'scheme_name', 'extraction_type', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'source', 'source_type', 'waterpoint_type', 'recorded_by', 'extraction_type_group', 'extraction_type_class', 'quantity_group', 'source_class', 'waterpoint_type_group', 'region_code', 'district_code', 'id', 'population'], axis=1, inplace=True)\n",
    "\n",
    "constr_y_train = clean_constr_train[~clean_constr_train['construction_year'].isnull()]['construction_year']\n",
    "constr_y_test = clean_constr_test[~clean_constr_test['construction_year'].isnull()]['construction_year']\n",
    "constr_y_total = pd.concat([constr_y_train, constr_y_test], axis=0, ignore_index=True).values\n",
    "\n",
    "constr_X_train = clean_constr_train[~clean_constr_train['construction_year'].isnull()][['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'public_meeting', 'permit']]\n",
    "constr_X_test = clean_constr_test[~clean_constr_test['construction_year'].isnull()][['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'public_meeting', 'permit']]\n",
    "constr_X_total = pd.concat([constr_X_train, constr_X_test], axis=0, ignore_index=True).values\n",
    "\n",
    "constr_X_train, constr_X_test, constr_y_train, constr_y_test = train_test_split(constr_X_total, constr_y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.44304013278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model_constr = RandomForestRegressor()\n",
    "model_constr.fit(constr_X_train, constr_y_train)\n",
    "mse = mean_squared_error(constr_y_test, model_constr.predict(constr_X_test))\n",
    "print np.sqrt(mse)\n",
    "\n",
    "prediction_df_train = clean_train[clean_train['construction_year'].isnull()]\n",
    "train_preds = model_constr.predict(prediction_df_train[['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'public_meeting', 'permit']].values)\n",
    "clean_train.ix[clean_train['construction_year'].isnull(), 'construction_year'] = train_preds\n",
    "\n",
    "prediction_df_test = clean_test[clean_test['construction_year'].isnull()]\n",
    "test_preds = model_constr.predict(prediction_df_test[['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'public_meeting', 'permit']].values)\n",
    "clean_test.ix[clean_test['construction_year'].isnull(), 'construction_year'] = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making new column that aggregates all other text columns.  This will be passed into a naive bayes classifier to be used as a feature later on.  The goal of using this technique is to capture some of the signal held by these columns without making dummies for all of them.  Not all columns are included, as some seem to add no new information.  The recorded_by column has the same value for every entry, thus is adding no information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train['text_cols'] = clean_train[['funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region', 'lga', 'ward', 'scheme_management', 'scheme_name', 'extraction_type', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'source', 'source_type', 'waterpoint_type']].values.tolist()\n",
    "clean_train.drop(['funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region', 'lga', 'ward', 'scheme_management', 'scheme_name', 'extraction_type', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'source', 'source_type', 'waterpoint_type', 'recorded_by', 'extraction_type_group', 'extraction_type_class', 'quantity_group', 'source_class', 'waterpoint_type_group', 'region_code', 'district_code'], axis=1, inplace=True)\n",
    "\n",
    "clean_test['text_cols'] = clean_test[['funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region', 'lga', 'ward', 'scheme_management', 'scheme_name', 'extraction_type', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'source', 'source_type', 'waterpoint_type']].values.tolist()\n",
    "clean_test.drop(['funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region', 'lga', 'ward', 'scheme_management', 'scheme_name', 'extraction_type', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'source', 'source_type', 'waterpoint_type', 'recorded_by', 'extraction_type_group', 'extraction_type_class', 'quantity_group', 'source_class', 'waterpoint_type_group', 'region_code', 'district_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train['text_cols'] = clean_train['text_cols'].map(lambda x: ' '.join(str(word) for word in x))\n",
    "clean_test['text_cols'] = clean_test['text_cols'].map(lambda x: ' '.join(str(word) for word in x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All columns other than 'text_cols' are now numerical.  The following is predicting the class of the well based on the text columns using naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "naive_y = clean_train['status_group'].values\n",
    "naive_X = clean_train['text_cols'].values\n",
    "naive_X_train, naive_X_test, naive_y_train, naive_y_test = train_test_split(naive_X, naive_y, stratify=naive_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_snow(doc):\n",
    "    snowball = SnowballStemmer('english')\n",
    "    return [snowball.stem(word) for word in word_tokenize(doc.lower())]\n",
    "\n",
    "def tokenize_port(doc):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in word_tokenize(doc.lower())]\n",
    "\n",
    "def tokenize_wordnet(doc):\n",
    "    word_net = WordNetLemmatizer()\n",
    "    return [word_net.lemmatize(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def make_model(tokenize_func):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize_func)\n",
    "    tfidf_vectorized = vectorizer.fit_transform(naive_X_train)\n",
    "    model = MultinomialNB()\n",
    "    model.fit(tfidf_vectorized, naive_y_train)\n",
    "    return model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695555555556\n"
     ]
    }
   ],
   "source": [
    "model_snow, vect_snow = make_model(tokenize_snow)\n",
    "print model_snow.score(vect_snow.transform(naive_X_test), naive_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_port, vect_port = make_model(tokenize_port)\n",
    "# print model_port.score(vect_port.transform(naive_X_test), naive_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_wordnet, vect_wordnet = make_model(tokenize_wordnet)\n",
    "# print model_wordnet.score(vect_wordnet.transform(naive_X_test), naive_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train['naive_bayes'] = model_snow.predict(vect_snow.transform(naive_X))\n",
    "clean_test['naive_bayes'] = model_snow.predict(vect_snow.transform(clean_test['text_cols'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train.drop('text_cols', axis=1, inplace=True)\n",
    "clean_test.drop('text_cols', axis=1, inplace=True)\n",
    "naive_dummies_train = pd.get_dummies(clean_train['naive_bayes'], drop_first=True)\n",
    "naive_dummies_test = pd.get_dummies(clean_test['naive_bayes'], drop_first=True)\n",
    "\n",
    "clean_train = pd.concat([clean_train, naive_dummies_train], axis=1)\n",
    "clean_train.drop('naive_bayes', axis=1, inplace=True)\n",
    "clean_test = pd.concat([clean_test, naive_dummies_test], axis=1)\n",
    "clean_test.drop('naive_bayes', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean_train['longitude'].fillna(value=clean_train['longitude'].mean(), inplace=True)\n",
    "# clean_test['longitude'].fillna(value=clean_test['longitude'].mean(), inplace=True)\n",
    "# clean_train['latitude'].fillna(value=clean_train['latitude'].mean(), inplace=True)\n",
    "# clean_test['latitude'].fillna(value=clean_test['latitude'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that population and construction year could be important features, so I am hesitant to drop them due to nans.  They also are quite variable, so I don't think it's a good idea to fill with mean or mode.  Thus, I am constructing K-nearest neighbors regressions for both of the columns and using it to fill the nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constr_y_train = clean_train[~clean_train['construction_year'].isnull()]['construction_year']\n",
    "# constr_y_test = clean_test[~clean_test['construction_year'].isnull()]['construction_year']\n",
    "# constr_y_total = pd.concat([constr_y_train, constr_y_test], axis=0, ignore_index=True).values\n",
    "\n",
    "# constr_X_train = clean_train[~clean_train['construction_year'].isnull()][['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'public_meeting', 'permit']]\n",
    "# constr_X_test = clean_test[~clean_test['construction_year'].isnull()][['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'public_meeting', 'permit']]\n",
    "# constr_X_total = pd.concat([constr_X_train, constr_X_test], axis=0, ignore_index=True).values\n",
    "\n",
    "# constr_X_train, constr_X_test, constr_y_train, constr_y_test = train_test_split(constr_X_total, constr_y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# model_constr = KNeighborsRegressor()\n",
    "# model_constr.fit(constr_X_train, constr_y_train)\n",
    "# mse = mean_squared_error(constr_y_test, model_constr.predict(constr_X_test))\n",
    "# print np.sqrt(mse)\n",
    "\n",
    "# prediction_df_train = clean_train[clean_train['construction_year'].isnull()]\n",
    "# train_preds = model_constr.predict(prediction_df_train[['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'public_meeting', 'permit']].values)\n",
    "# clean_train.ix[clean_train['construction_year'].isnull(), 'construction_year'] = train_preds\n",
    "\n",
    "# prediction_df_test = clean_test[clean_test['construction_year'].isnull()]\n",
    "# test_preds = model_constr.predict(prediction_df_test[['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'public_meeting', 'permit']].values)\n",
    "# clean_test.ix[clean_test['construction_year'].isnull(), 'construction_year'] = test_preds\n",
    "\n",
    "# clean_train.info()\n",
    "# clean_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train.drop(['population'], axis=1, inplace=True)\n",
    "clean_test.drop(['population'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train.to_csv('../data/processed/clean_train_df.csv')\n",
    "clean_test.to_csv('../data/processed/clean_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
